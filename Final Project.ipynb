{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: A/B Test - Free Trial Screener\n",
    "Author: Wei Chong Ong\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#design\">Experiment Design</a></li>\n",
    "<li><a href=\"#analysis\">Experiment Analysis</a></li>\n",
    "<li><a href=\"#follow-up\">Follow-Up Experiment</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm, binom_test\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_control = pd.read_csv('Final Project Results - Control.csv')\n",
    "df_experiment = pd.read_csv('Final Project Results - Experiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Pageviews_cont</th>\n",
       "      <th>Clicks_cont</th>\n",
       "      <th>Enrollments_cont</th>\n",
       "      <th>Payments_cont</th>\n",
       "      <th>Pageviews_exp</th>\n",
       "      <th>Clicks_exp</th>\n",
       "      <th>Enrollments_exp</th>\n",
       "      <th>Payments_exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat, Oct 11</td>\n",
       "      <td>7723</td>\n",
       "      <td>687</td>\n",
       "      <td>134.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7716</td>\n",
       "      <td>686</td>\n",
       "      <td>105.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sun, Oct 12</td>\n",
       "      <td>9102</td>\n",
       "      <td>779</td>\n",
       "      <td>147.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>9288</td>\n",
       "      <td>785</td>\n",
       "      <td>116.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon, Oct 13</td>\n",
       "      <td>10511</td>\n",
       "      <td>909</td>\n",
       "      <td>167.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>10480</td>\n",
       "      <td>884</td>\n",
       "      <td>145.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue, Oct 14</td>\n",
       "      <td>9871</td>\n",
       "      <td>836</td>\n",
       "      <td>156.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>9867</td>\n",
       "      <td>827</td>\n",
       "      <td>138.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed, Oct 15</td>\n",
       "      <td>10014</td>\n",
       "      <td>837</td>\n",
       "      <td>163.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>9793</td>\n",
       "      <td>832</td>\n",
       "      <td>140.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thu, Oct 16</td>\n",
       "      <td>9670</td>\n",
       "      <td>823</td>\n",
       "      <td>138.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>9500</td>\n",
       "      <td>788</td>\n",
       "      <td>129.0</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fri, Oct 17</td>\n",
       "      <td>9008</td>\n",
       "      <td>748</td>\n",
       "      <td>146.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>9088</td>\n",
       "      <td>780</td>\n",
       "      <td>127.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sat, Oct 18</td>\n",
       "      <td>7434</td>\n",
       "      <td>632</td>\n",
       "      <td>110.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>7664</td>\n",
       "      <td>652</td>\n",
       "      <td>94.0</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sun, Oct 19</td>\n",
       "      <td>8459</td>\n",
       "      <td>691</td>\n",
       "      <td>131.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>8434</td>\n",
       "      <td>697</td>\n",
       "      <td>120.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mon, Oct 20</td>\n",
       "      <td>10667</td>\n",
       "      <td>861</td>\n",
       "      <td>165.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>10496</td>\n",
       "      <td>860</td>\n",
       "      <td>153.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Tue, Oct 21</td>\n",
       "      <td>10660</td>\n",
       "      <td>867</td>\n",
       "      <td>196.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>10551</td>\n",
       "      <td>864</td>\n",
       "      <td>143.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wed, Oct 22</td>\n",
       "      <td>9947</td>\n",
       "      <td>838</td>\n",
       "      <td>162.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>9737</td>\n",
       "      <td>801</td>\n",
       "      <td>128.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thu, Oct 23</td>\n",
       "      <td>8324</td>\n",
       "      <td>665</td>\n",
       "      <td>127.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>8176</td>\n",
       "      <td>642</td>\n",
       "      <td>122.0</td>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fri, Oct 24</td>\n",
       "      <td>9434</td>\n",
       "      <td>673</td>\n",
       "      <td>220.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>9402</td>\n",
       "      <td>697</td>\n",
       "      <td>194.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sat, Oct 25</td>\n",
       "      <td>8687</td>\n",
       "      <td>691</td>\n",
       "      <td>176.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>8669</td>\n",
       "      <td>669</td>\n",
       "      <td>127.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sun, Oct 26</td>\n",
       "      <td>8896</td>\n",
       "      <td>708</td>\n",
       "      <td>161.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>8881</td>\n",
       "      <td>693</td>\n",
       "      <td>153.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Mon, Oct 27</td>\n",
       "      <td>9535</td>\n",
       "      <td>759</td>\n",
       "      <td>233.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>9655</td>\n",
       "      <td>771</td>\n",
       "      <td>213.0</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Tue, Oct 28</td>\n",
       "      <td>9363</td>\n",
       "      <td>736</td>\n",
       "      <td>154.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>9396</td>\n",
       "      <td>736</td>\n",
       "      <td>162.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wed, Oct 29</td>\n",
       "      <td>9327</td>\n",
       "      <td>739</td>\n",
       "      <td>196.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>9262</td>\n",
       "      <td>727</td>\n",
       "      <td>201.0</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Thu, Oct 30</td>\n",
       "      <td>9345</td>\n",
       "      <td>734</td>\n",
       "      <td>167.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9308</td>\n",
       "      <td>728</td>\n",
       "      <td>207.0</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Fri, Oct 31</td>\n",
       "      <td>8890</td>\n",
       "      <td>706</td>\n",
       "      <td>174.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>8715</td>\n",
       "      <td>722</td>\n",
       "      <td>182.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sat, Nov 1</td>\n",
       "      <td>8460</td>\n",
       "      <td>681</td>\n",
       "      <td>156.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>8448</td>\n",
       "      <td>695</td>\n",
       "      <td>142.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sun, Nov 2</td>\n",
       "      <td>8836</td>\n",
       "      <td>693</td>\n",
       "      <td>206.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>8836</td>\n",
       "      <td>724</td>\n",
       "      <td>182.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mon, Nov 3</td>\n",
       "      <td>9437</td>\n",
       "      <td>788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9359</td>\n",
       "      <td>789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Tue, Nov 4</td>\n",
       "      <td>9420</td>\n",
       "      <td>781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9427</td>\n",
       "      <td>743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Wed, Nov 5</td>\n",
       "      <td>9570</td>\n",
       "      <td>805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9633</td>\n",
       "      <td>808</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Thu, Nov 6</td>\n",
       "      <td>9921</td>\n",
       "      <td>830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9842</td>\n",
       "      <td>831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Fri, Nov 7</td>\n",
       "      <td>9424</td>\n",
       "      <td>781</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9272</td>\n",
       "      <td>767</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sat, Nov 8</td>\n",
       "      <td>9010</td>\n",
       "      <td>756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8969</td>\n",
       "      <td>760</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sun, Nov 9</td>\n",
       "      <td>9656</td>\n",
       "      <td>825</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9697</td>\n",
       "      <td>850</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Mon, Nov 10</td>\n",
       "      <td>10419</td>\n",
       "      <td>874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10445</td>\n",
       "      <td>851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Tue, Nov 11</td>\n",
       "      <td>9880</td>\n",
       "      <td>830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9931</td>\n",
       "      <td>831</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Wed, Nov 12</td>\n",
       "      <td>10134</td>\n",
       "      <td>801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10042</td>\n",
       "      <td>802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Thu, Nov 13</td>\n",
       "      <td>9717</td>\n",
       "      <td>814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9721</td>\n",
       "      <td>829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Fri, Nov 14</td>\n",
       "      <td>9192</td>\n",
       "      <td>735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9304</td>\n",
       "      <td>770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sat, Nov 15</td>\n",
       "      <td>8630</td>\n",
       "      <td>743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8668</td>\n",
       "      <td>724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Sun, Nov 16</td>\n",
       "      <td>8970</td>\n",
       "      <td>722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8988</td>\n",
       "      <td>710</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Pageviews_cont  Clicks_cont  Enrollments_cont  Payments_cont  \\\n",
       "0   Sat, Oct 11            7723          687             134.0           70.0   \n",
       "1   Sun, Oct 12            9102          779             147.0           70.0   \n",
       "2   Mon, Oct 13           10511          909             167.0           95.0   \n",
       "3   Tue, Oct 14            9871          836             156.0          105.0   \n",
       "4   Wed, Oct 15           10014          837             163.0           64.0   \n",
       "5   Thu, Oct 16            9670          823             138.0           82.0   \n",
       "6   Fri, Oct 17            9008          748             146.0           76.0   \n",
       "7   Sat, Oct 18            7434          632             110.0           70.0   \n",
       "8   Sun, Oct 19            8459          691             131.0           60.0   \n",
       "9   Mon, Oct 20           10667          861             165.0           97.0   \n",
       "10  Tue, Oct 21           10660          867             196.0          105.0   \n",
       "11  Wed, Oct 22            9947          838             162.0           92.0   \n",
       "12  Thu, Oct 23            8324          665             127.0           56.0   \n",
       "13  Fri, Oct 24            9434          673             220.0          122.0   \n",
       "14  Sat, Oct 25            8687          691             176.0          128.0   \n",
       "15  Sun, Oct 26            8896          708             161.0          104.0   \n",
       "16  Mon, Oct 27            9535          759             233.0          124.0   \n",
       "17  Tue, Oct 28            9363          736             154.0           91.0   \n",
       "18  Wed, Oct 29            9327          739             196.0           86.0   \n",
       "19  Thu, Oct 30            9345          734             167.0           75.0   \n",
       "20  Fri, Oct 31            8890          706             174.0          101.0   \n",
       "21   Sat, Nov 1            8460          681             156.0           93.0   \n",
       "22   Sun, Nov 2            8836          693             206.0           67.0   \n",
       "23   Mon, Nov 3            9437          788               NaN            NaN   \n",
       "24   Tue, Nov 4            9420          781               NaN            NaN   \n",
       "25   Wed, Nov 5            9570          805               NaN            NaN   \n",
       "26   Thu, Nov 6            9921          830               NaN            NaN   \n",
       "27   Fri, Nov 7            9424          781               NaN            NaN   \n",
       "28   Sat, Nov 8            9010          756               NaN            NaN   \n",
       "29   Sun, Nov 9            9656          825               NaN            NaN   \n",
       "30  Mon, Nov 10           10419          874               NaN            NaN   \n",
       "31  Tue, Nov 11            9880          830               NaN            NaN   \n",
       "32  Wed, Nov 12           10134          801               NaN            NaN   \n",
       "33  Thu, Nov 13            9717          814               NaN            NaN   \n",
       "34  Fri, Nov 14            9192          735               NaN            NaN   \n",
       "35  Sat, Nov 15            8630          743               NaN            NaN   \n",
       "36  Sun, Nov 16            8970          722               NaN            NaN   \n",
       "\n",
       "    Pageviews_exp  Clicks_exp  Enrollments_exp  Payments_exp  \n",
       "0            7716         686            105.0          34.0  \n",
       "1            9288         785            116.0          91.0  \n",
       "2           10480         884            145.0          79.0  \n",
       "3            9867         827            138.0          92.0  \n",
       "4            9793         832            140.0          94.0  \n",
       "5            9500         788            129.0          61.0  \n",
       "6            9088         780            127.0          44.0  \n",
       "7            7664         652             94.0          62.0  \n",
       "8            8434         697            120.0          77.0  \n",
       "9           10496         860            153.0          98.0  \n",
       "10          10551         864            143.0          71.0  \n",
       "11           9737         801            128.0          70.0  \n",
       "12           8176         642            122.0          68.0  \n",
       "13           9402         697            194.0          94.0  \n",
       "14           8669         669            127.0          81.0  \n",
       "15           8881         693            153.0         101.0  \n",
       "16           9655         771            213.0         119.0  \n",
       "17           9396         736            162.0         120.0  \n",
       "18           9262         727            201.0          96.0  \n",
       "19           9308         728            207.0          67.0  \n",
       "20           8715         722            182.0         123.0  \n",
       "21           8448         695            142.0         100.0  \n",
       "22           8836         724            182.0         103.0  \n",
       "23           9359         789              NaN           NaN  \n",
       "24           9427         743              NaN           NaN  \n",
       "25           9633         808              NaN           NaN  \n",
       "26           9842         831              NaN           NaN  \n",
       "27           9272         767              NaN           NaN  \n",
       "28           8969         760              NaN           NaN  \n",
       "29           9697         850              NaN           NaN  \n",
       "30          10445         851              NaN           NaN  \n",
       "31           9931         831              NaN           NaN  \n",
       "32          10042         802              NaN           NaN  \n",
       "33           9721         829              NaN           NaN  \n",
       "34           9304         770              NaN           NaN  \n",
       "35           8668         724              NaN           NaN  \n",
       "36           8988         710              NaN           NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_control.merge(df_experiment, on='Date', suffixes=('_cont', '_exp'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='design'></a>\n",
    "# Experiment Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Invariant Metrics**  \n",
    "Invariant metrics are the metrics that shouldn’t change across our experiment and control when we run our experiment.\n",
    "- **Number of cookies**: The number of unique cookies that view the course overview page. This is a population sizing metric. Since the unit of diversion is cookie, the cookies are being randomly assigned to the experiment and control groups. So, we should definitely have roughly the same number of cookies in each group. \n",
    "- **Number of clicks**: The number of unique cookies that click the “Start free trial\" button, which happens before the free trial screener is trigger. Since cookies click the \"Start free trial\" button before they were asked how much time they had available to devote to the course, this metric shouldn't be affected by the change.\n",
    "- **Click-through-probability**: The number of unique cookies to click the \"Start free trial\" button divided by number of unique cookies to view the course overview page. Since this metric is calculated from the two invariant metrics above, this metric shouldn't change as well.\n",
    "\n",
    "**Evaluation Metrics**  \n",
    "Evaluation metrics are the metrics in which we expect to see a change, and are relevant to the business goals we aim to achieve. In this case, the business goal of Udacity is to improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course.\n",
    "- **Gross conversion**: The number of user-ids to complete checkout and enroll in the free trial divided by number of unique cookies to click the \"Start free trial\" button. This metric should change if the screener does affect the number of enrollments and we would assume this metric to decrease, by reducing the number of enrolled users that have left the free trial.\n",
    "- **Retention**: The number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. If having the screener does affect the number of enrolled users leaving the free trial, we would expect this metric to increase by increasing the number users passing the free trial and making payment.\n",
    "- **Net conversion**: The number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by the number of unique cookies to click the \"Start free trial\" button. In contrast to the gross conversion, we would expect this metric to remain the same, or increase, because we do not want the screener to significantly reduce the number of students to continue past the free trial and eventually complete the course.\n",
    "\n",
    "Number of user-ids: This is not a good invariant metric because it is expected to reduce as a result of the experiment. It is an applicable evaluation metric because it would record the number of students continue past the free trial, but it's not the best metric because it's not normalized.\n",
    "\n",
    "`Any place \"unique cookies\" are mentioned, the uniqueness is determined by day. (That is, the same cookie visiting on different days would be counted twice.) User-ids are automatically unique since the site does not allow the same user-id to enroll twice.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would a change in any one of your evaluation metrics be sufficient? Would you want to see multiple metrics all move or not move at the same time in order to launch? \n",
    "\n",
    "> There are two gates to pass in order to be sure that the experiment is a success and launch the change:\n",
    "1. The first criterion is that the experiment group must have a statistically and practically significant **lower gross conversion** than the control group. It shows that the screener does has effect on reducing the number of enrollments. \n",
    "2. Then, the second criterion is that there is a significantly **higher retention** in experiment group and the **net conversion** should at least remain the **same** or **does not decrease significantly**. Due to the lower number of enrollments in experiment group, if the results shows similar retention in both groups, it will lead to a decrease in net conversion. Therefore, both control and experiment group should have at least similar number of payments, that is, without significantly reducing the number of students to continue past the free trial, so that the net conversion does not decrease. If there is a significant increase in net conversion, that's good, but this is not the criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypothesis was that the free trial screener sets clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough time, without significantly reducing the number of students to continue past the free trial and eventually complete the course. Since this statement involves several evaluation metrics, we can define the null and alternative hypotheses for each of the metrics:\n",
    "\n",
    "**Gross Conversion:** \n",
    "**$$H_0: CVR_{exp}-CVR_{cont} \\ge 0$$**\n",
    "**$$H_1: CVR_{exp}-CVR_{cont} \\lt 0$$**\n",
    "\n",
    "**Retention:** \n",
    "**$$H_0: CVR_{exp}-CVR_{cont} \\le 0$$**\n",
    "**$$H_1: CVR_{exp}-CVR_{cont} \\gt 0$$**\n",
    "\n",
    "**Net Conversion:** \n",
    "**$$H_0: CVR_{exp}-CVR_{cont} \\le 0$$**\n",
    "**$$H_1: CVR_{exp}-CVR_{cont} \\gt 0$$**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Variability of Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe contains rough estimates of the baseline values for the following metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unique cookies to view course overview page per day:</td>\n",
       "      <td>40000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unique cookies to click \"Start free trial\" per day:</td>\n",
       "      <td>3200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enrollments per day:</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Click-through-probability on \"Start free trial\":</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Probability of enrolling, given click:</td>\n",
       "      <td>0.206250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probability of payment, given enroll:</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Probability of payment, given click</td>\n",
       "      <td>0.109313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Metric         Value\n",
       "0  Unique cookies to view course overview page per day:  40000.000000\n",
       "1   Unique cookies to click \"Start free trial\" per day:   3200.000000\n",
       "2                                  Enrollments per day:    660.000000\n",
       "3      Click-through-probability on \"Start free trial\":      0.080000\n",
       "4                Probability of enrolling, given click:      0.206250\n",
       "5                 Probability of payment, given enroll:      0.530000\n",
       "6                   Probability of payment, given click      0.109313"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_baseline_values = pd.read_csv('Final Project Baseline Values.csv', header=None, names=['Metric', 'Value'])\n",
    "pd.options.display.max_colwidth = 100\n",
    "df_baseline_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_views = df_baseline_values['Value'][0]\n",
    "clicks = df_baseline_values['Value'][1]\n",
    "enroll = df_baseline_values['Value'][2]\n",
    "gross_cvr = df_baseline_values['Value'][4]\n",
    "retent = df_baseline_values['Value'][5]\n",
    "net_cvr = df_baseline_values['Value'][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic Estimate of Variability\n",
    "Estimate the standard error of the evaluation metrics using a sample size of 5000 cookies visiting the course overview page and the baseline values. Since the evaluation metrics are probabilities, we can assume a binomial distribution.\n",
    "\n",
    "$$SE = \\sqrt{\\frac{p*(1-p}{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard error of gross conversion = 0.0202\n",
      "Standard error of retention = 0.0549\n",
      "Standard error of net conversion = 0.0156\n"
     ]
    }
   ],
   "source": [
    "sample_page_views = 5000\n",
    "sample_clicks = clicks / page_views * sample_page_views\n",
    "sample_enroll = enroll / page_views * sample_page_views\n",
    "\n",
    "# Compute standard error (Binomial)\n",
    "SE_gross = np.sqrt(gross_cvr * (1 - gross_cvr) / sample_clicks)\n",
    "SE_retent = np.sqrt(retent * (1 - retent) / sample_enroll)\n",
    "SE_net = np.sqrt(net_cvr * (1 - net_cvr) / sample_clicks)\n",
    "\n",
    "print('Standard error of gross conversion = {:.4f}'.format(SE_gross))\n",
    "print('Standard error of retention = {:.4f}'.format(SE_retent))\n",
    "print('Standard error of net conversion = {:.4f}'.format(SE_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the unit of analysis for gross conversion and net conversion is same as the unit of conversion, their analytical variability will be similar to the empirical variability. However, for retention, the analytic variability is likely to be an under-estimate because its unit of analysis (user-id) is different from the unit of diversion (cookie). The reason is that when we are doing cookie-based diversion, every single cookie is a different random draw, and so the independence assumption of the analytical calculation is actually valid. This independence assumption is no longer valid when we apply to retention because a user id could consist of multiple cookies. The cookies within a user id are actually correlated together. That would increase the variability greatly. Therefore, we would want to collect an empirical estimate of variability of retention if we had time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Samples vs. Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the required sample size, i.e, total number of pageviews (across both groups) to adequately power the experiment. We want to make sure that we have enough power for each metric by using an alpha of 0.05 and a beta of 0.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Detectable Effect for each Evaluation Metrics\n",
    "The difference that would have to be observed before that was a meaningful change for the business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmin_gross = 0.01\n",
    "dmin_retent = 0.01\n",
    "dmin_net = 0.0075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Calculate required sample size iteratively using analytics estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute critical z score\n",
    "def get_z_crit(alpha):\n",
    "    return -norm.ppf(alpha / 2)\n",
    "\n",
    "\n",
    "# s is the pooled standard error for N=1 in each group, which is sqrt(p*(1-p)*(1/1 + 1/1))\n",
    "def get_beta(z_crit, s, d_min, N):\n",
    "    SE = s / np.sqrt(N)\n",
    "    return norm.cdf(z_crit * SE, loc=d_min, scale=SE)\n",
    "\n",
    "\n",
    "# Compute the required sample size for the experiment\n",
    "def required_size(s, d_min, alpha=0.05, beta=0.2):\n",
    "    N = 1\n",
    "    dmin = abs(get_beta(get_z_crit(alpha), s, d_min, N) - beta)\n",
    "    for N in range(2, 40000):\n",
    "        d = abs(get_beta(get_z_crit(alpha), s, d_min, N) - beta)\n",
    "        if d < dmin:\n",
    "            dmin = d\n",
    "            size = N\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gross Conversion\n",
      "clicks: 51398\n",
      "pageviews: 642474\n",
      "\n",
      "Retention\n",
      "enrollments: 78206\n",
      "pageviews: 4739756\n",
      "\n",
      "Net Conversion\n",
      "clicks: 54342\n",
      "pageviews: 679274\n"
     ]
    }
   ],
   "source": [
    "sample_clicks_gross_conversion_per_group = required_size(s=SE_gross * np.sqrt(sample_clicks) * np.sqrt(2), d_min=dmin_gross)\n",
    "sample_page_views_gross_conversion_per_group = sample_clicks_gross_conversion_per_group * page_views / clicks\n",
    "print('Gross Conversion')\n",
    "print('clicks: {}' .format(int(sample_clicks_gross_conversion_per_group) * 2))\n",
    "print('pageviews: {}' .format(int(sample_page_views_gross_conversion_per_group) * 2))\n",
    "\n",
    "print('')\n",
    "\n",
    "sample_enroll_retention_per_group = required_size(s=SE_retent * np.sqrt(sample_enroll) * np.sqrt(2), d_min=dmin_retent)\n",
    "sample_page_views_retention_per_group = sample_enroll_retention_per_group * page_views / enroll\n",
    "print('Retention')\n",
    "print('enrollments: {}' .format(int(sample_enroll_retention_per_group) * 2))\n",
    "print('pageviews: {}' .format(int(sample_page_views_retention_per_group) * 2))\n",
    "\n",
    "print('')\n",
    "\n",
    "sample_clicks_net_conversion_per_group = required_size(s=SE_net * np.sqrt(sample_clicks) * np.sqrt(2), d_min=dmin_net)\n",
    "sample_page_views_net_conversion_per_group = sample_clicks_net_conversion_per_group * page_views / clicks\n",
    "print('Net Conversion')\n",
    "print('clicks: {}' .format(int(sample_clicks_net_conversion_per_group) * 2))\n",
    "print('pageviews: {}' .format(int(sample_page_views_net_conversion_per_group) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Calculate required sample size using [Online Calculator](https://www.evanmiller.org/ab-testing/sample-size.html) or [Equation (2.33)](http://vanbelle.org/chapters%5Cwebchapter2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gross Conversion:**  \n",
    "clicks = 51670  \n",
    "pageviews = 645875\n",
    "\n",
    "**Retention:**  \n",
    "enrollments = 78230  \n",
    "pageviews = 4741212\n",
    "\n",
    "**Net Conversion:**  \n",
    "clicks = 54826  \n",
    "pageviews = 685325\n",
    "\n",
    "-> Pageviews Required for the experiement: 4741212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration vs. Exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What fraction of traffic you would divert to this experiment and, given this, how many days you would need to run the experiment.\n",
    "\n",
    "> Assuming there were no other experiments we wanted to run simultaneously, we could divert 100% of the traffic to our experiment. Given 40,000 page views per day, the experiment would take about 119 days, which is unreasonably long to run an experiment. This might cause some potential problems for Udacity. First, we cannot perfom any other experiment during this period (opportunity costs). Secondly, if the change harms the user experience (frustrated students, inefficient coaching resources) and decreases conversion rates, we won't notice it (or cannot really say so) for more than four months (business risk). Therefore, it seems more reasonable to only test the gross conversion and net conversion and to eliminate retention as an evaluation metric. Then, our required sample size would be much smaller and it will take about 17 days to run the experiment with 100% diversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the change risky enough that you wouldn't want to run on all traffic?\n",
    "\n",
    "> Since the experiment does not involve a feature that is critical with regards to potential media coverage, it  does not seem to be very risky and we could divert a high percentage of traffic to the experiment. Still, since there is always the potential that something goes wrong during implemention, we may not want to divert all of our traffic to it. 80% of the traffic (22 days) seems to be quite reasonable. The rest of the traffic could be then diverted to other comparable experiements and run them at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration (Retention, 100% traffic) : 118.5 days\n",
      "Duration (Net Conversion, 100% traffic): 17.1 days\n"
     ]
    }
   ],
   "source": [
    "# fraction of daily traffic -> 1 = All daily traffic\n",
    "exposure = 1        \n",
    "\n",
    "# Retention\n",
    "duration = 4741212 / (page_views * exposure)\n",
    "print('Duration (Retention, 100% traffic) : {:.1f} days' .format(duration))\n",
    "\n",
    "# Net Conversion\n",
    "duration = 685325 / (page_views * exposure)\n",
    "print('Duration (Net Conversion, 100% traffic): {:.1f} days' .format(duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration (Net Conversion, 80% traffic): 21.4 days\n"
     ]
    }
   ],
   "source": [
    "# 80% of the daily traffic\n",
    "exposure = 0.8\n",
    "\n",
    "# Net Conversion\n",
    "duration = 685325 / (page_views * exposure)\n",
    "print('Duration (Net Conversion, 80% traffic): {:.1f} days' .format(duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analysis'></a>\n",
    "# Experiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the experiment was run properly, we first need to make sure that all invariant metrics pass the sanity check. We would expect that these metrics do not differ significantly between control and experiment group. Otherwise, this would imply that someting is wrong with the experiment setup and that our results are biased. We might then have to look at the day by day data and see if we can offer any insight into what is causing the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check: Population sizing invariants\n",
    "Check whether the population sizing invariant metrics are equivalent between the two groups. Since the invariant metrics (number of cookies and number of clicks) are a simple count that should be randomly split between the two groups, we can use a binomial test. As the sample size is large, we can assume that the sampling distribution of the sample proportion approximates a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected proprotion (evenly split between the two gropus)\n",
    "actual_prop = 0.5\n",
    "\n",
    "def population_sizing_invariant(N_cont, N_exp, p, alpha=0.05):\n",
    "    \n",
    "    # Compute standard deviation of the sampling distribution for the proportion (standard error) of 0.5\n",
    "    SE = np.sqrt(p * (1 - p) / (N_cont + N_exp))\n",
    "    \n",
    "    # Compute margin of error with 95% confidence interval\n",
    "    MOE = SE * get_z_crit(alpha)\n",
    "    lb, ub = p - MOE, p + MOE\n",
    "    \n",
    "    # Observed proportion\n",
    "    phat = N_cont / (N_cont + N_exp)\n",
    "    print('Observed proportion: {:.4f}'.format(phat))\n",
    "    print('Confidence Interval:[{:.4f},{:.4f}]'.format(lb, ub))\n",
    "    \n",
    "    # Check if the observed proportion falls within the confidence interval\n",
    "    if (phat >= lb) & (phat <= ub):\n",
    "        return print('-> Sanity check passed')\n",
    "    else:\n",
    "        return print('-> Sanity check failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cookies that views the page\n",
      "Observed proportion: 0.5006\n",
      "Confidence Interval:[0.4988,0.5012]\n",
      "-> Sanity check passed\n",
      "\n",
      "Number of clicks on \"Start free trial\"\n",
      "Observed proportion: 0.5005\n",
      "Confidence Interval:[0.4959,0.5041]\n",
      "-> Sanity check passed\n"
     ]
    }
   ],
   "source": [
    "print('Number of cookies that views the page')\n",
    "population_sizing_invariant(df['Pageviews_cont'].sum(), df['Pageviews_exp'].sum(), actual_prop)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Number of clicks on \"Start free trial\"')\n",
    "population_sizing_invariant(df['Clicks_cont'].sum(), df['Clicks_exp'].sum(), actual_prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 1: compute p-value using z test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Sanity check passed\n"
     ]
    }
   ],
   "source": [
    "stat, pval_ztest = sm.stats.proportions_ztest(df['Pageviews_cont'].sum(), df['Pageviews_exp'].sum() + df['Pageviews_cont'].sum(), value = 0.5, alternative = 'two-sided')\n",
    "\n",
    "if pval_ztest > 0.05:\n",
    "    print('-> Sanity check passed')\n",
    "else:\n",
    "    print('-> Sanity check failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 2: compute p-value using the exact binomial test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Sanity check passed\n"
     ]
    }
   ],
   "source": [
    "pval_binom = binom_test(df['Pageviews_cont'].sum(), df['Pageviews_exp'].sum() + df['Pageviews_cont'].sum(), p=0.5, alternative='two-sided')\n",
    "\n",
    "if pval_binom > 0.05:\n",
    "    print('-> Sanity check passed')\n",
    "else:\n",
    "    print('-> Sanity check failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check: Click-Through Probability invariants\n",
    "Check whether the observed difference in click-through probability between control and experiment group falls within the 95% confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected difference in click-through probability between the two groups\n",
    "actual_diff = 0\n",
    "\n",
    "def CTP_invariant(N_cont, N_exp, X_cont, X_exp, d, alpha=0.05):\n",
    "    \n",
    "    # Compute pooled standard error assuming that both of the variances from the two samples are equal\n",
    "    p_pool = (X_cont + X_exp) / (N_cont + N_exp)\n",
    "    SE_pool = np.sqrt(p_pool * (1 - p_pool) * (1 / N_cont + 1 / N_exp))\n",
    "    \n",
    "    # Compute margin of error with 95% confidence interval\n",
    "    MOE = SE_pool * get_z_crit(alpha)\n",
    "    lb, ub = d - MOE, d + MOE\n",
    "    \n",
    "    # Observed difference in click-through probability\n",
    "    dhat = (X_exp / N_exp) - (X_cont / N_cont)\n",
    "    print('Observed difference in proportion: {:.4f}'.format(dhat))\n",
    "    print('Confidence Interval:[{:.4f},{:.4f}]'.format(lb, ub))\n",
    "    \n",
    "    # Check if the observed difference in click-through probability falls within the confidence interval\n",
    "    if (dhat >= lb) & (dhat <= ub):\n",
    "        return print('-> Sanity check passed')\n",
    "    else:\n",
    "        return print('-> Sanity check passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicks-through probabiliy on \"Start free trial\"\n",
      "Observed difference in proportion: 0.0001\n",
      "Confidence Interval:[-0.0013,0.0013]\n",
      "-> Sanity check passed\n"
     ]
    }
   ],
   "source": [
    "print('Clicks-through probabiliy on \"Start free trial\"')\n",
    "CTP_invariant(df['Pageviews_cont'].sum(), df['Pageviews_exp'].sum(), df['Clicks_cont'].sum(), df['Clicks_exp'].sum(), actual_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: compute p-value using z test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Sanity check passed\n"
     ]
    }
   ],
   "source": [
    "n_clicks = np.array([df['Clicks_cont'].sum(), df['Clicks_exp'].sum()])\n",
    "n = np.array([df['Pageviews_cont'].sum(), df['Pageviews_exp'].sum()])\n",
    "stat, pval_ztest = sm.stats.proportions_ztest(n_clicks, n, alternative = 'two-sided')\n",
    "\n",
    "if pval_ztest > 0.05:\n",
    "    print('-> Sanity check passed')\n",
    "else:\n",
    "    print('-> Sanity check failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis and Intepretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for Practical and Statistical Significance\n",
    "Next, for our evaluation metrics, calculate a 95% confidence interval for the difference between the experiment and control groups, and check whether each metric is statistically and/or practically significance. \n",
    "\n",
    "A metric is statistically significant if the confidence interval does not include 0 (that is, we can be confident there was a change), and it is practically significant if the confidence interval does not include the practical significance boundary (that is, we can be confident there is a change that matters to the business.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect Size Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_size_test(N_cont, N_exp, X_cont, X_exp, dtrue, dmin, alpha=0.05):\n",
    "    \n",
    "    # Compute pooled standard error assuming that both of the variances from the two samples are equal\n",
    "    p_pool = (X_cont + X_exp) / (N_cont + N_exp)\n",
    "    SE_pool = np.sqrt(p_pool * (1 - p_pool) * (1 / N_cont + 1 / N_exp))\n",
    "    \n",
    "    # Compute margin of error with 95% confidence interval\n",
    "    MOE = SE_pool * get_z_crit(alpha)\n",
    "    \n",
    "    # Observed difference in proportion\n",
    "    dhat = (X_exp / N_exp) - (X_cont / N_cont)\n",
    "    lb, ub = dhat - MOE, dhat + MOE\n",
    "    print('Observed difference in proportion: {:.4f}'.format(dhat))\n",
    "    print('Confidence Interval:[{:.4f},{:.4f}]'.format(lb, ub))\n",
    "    \n",
    "    # Check if it is statistically and practically significant\n",
    "    if (dtrue >= lb) & (dtrue <= ub):\n",
    "        print('-> Not statistical significant')\n",
    "    else:\n",
    "        print('-> Statistical significant')\n",
    "    \n",
    "    if (dmin >= lb) & (dmin <= ub):\n",
    "        print('Not practical significant')\n",
    "    elif (-dmin >= lb) & (-dmin <= ub):\n",
    "        print('-> Not practical significant')\n",
    "    else:\n",
    "        print('-> Practical significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gross Conversion\n",
      "Observed difference in proportion: -0.0206\n",
      "Confidence Interval:[-0.0291,-0.0120]\n",
      "-> Statistical significant\n",
      "-> Practical significant\n",
      "\n",
      "Net Conversion\n",
      "Observed difference in proportion: -0.0049\n",
      "Confidence Interval:[-0.0116,0.0019]\n",
      "-> Not statistical significant\n",
      "-> Not practical significant\n"
     ]
    }
   ],
   "source": [
    "print('Gross Conversion')\n",
    "effect_size_test(df['Clicks_cont'][:23].sum(), df['Clicks_exp'][:23].sum(), df['Enrollments_cont'].sum(), df['Enrollments_exp'].sum(), actual_diff, dmin_gross)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Net Conversion')\n",
    "effect_size_test(df['Clicks_cont'][:23].sum(), df['Clicks_exp'][:23].sum(), df['Payments_cont'].sum(), df['Payments_exp'].sum(), actual_diff, dmin_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sign Tests\n",
    "For each evaluation metric, do a sign test using the day-by-day breakdown and report the p-value of the sign test and whether the result is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the difference in gross conversion between two groups\n",
    "df['diff_gross'] = df['Enrollments_exp'] / df['Clicks_exp'] - df['Enrollments_cont'] / df['Clicks_cont']\n",
    "\n",
    "# Compute the difference in net conversion between two groups\n",
    "df['diff_net'] = df['Payments_exp'] / df['Clicks_exp'] - df['Payments_cont'] / df['Clicks_cont']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_test(n, k, p):\n",
    "    # n = total number of days\n",
    "    # k = total number of days with negative change (experiment - control)\n",
    "    p_value = binom_test(k, n, p, alternative='two-sided')\n",
    "    \n",
    "    # Check if it is statistically and practically significant\n",
    "    print('p-value: {:.4f}'.format(p_value))\n",
    "    if (p_value < 0.05):\n",
    "        print('-> Statistically significant')\n",
    "    else:\n",
    "        print('-> Not statistically significant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gross Conversion\n",
      "p-value: 0.0026\n",
      "-> Statistically significant\n",
      "\n",
      "Net Conversion\n",
      "p-value: 0.6776\n",
      "-> Not statistically significant\n"
     ]
    }
   ],
   "source": [
    "# Both observed gross and net conversion are negative\n",
    "# If there was no difference, then there would be a 50% chance of a negative change on each day.\n",
    "\n",
    "print('Gross Conversion')\n",
    "sign_test(df['Enrollments_cont'].count(), (df['diff_gross']<0).sum(), 0.5)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Net Conversion')\n",
    "sign_test(df['Payments_cont'].count(), (df['diff_net']<0).sum(), 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Interpretation \n",
    "The Bonferroni correction was not used because we want both the evaluation metrics to pass the practical significant boundary in order to launch the change. The Bonferroni correction is useful in reducing Type I errors (Deciding to launch the change when there is actually not a significant difference), and not necessarily effective in reducing Type II errors (Deciding not to launch the change when there is actually a significant difference).\n",
    "\n",
    "Both the Effect Size Tests and Sign Tests have produced same results. There is both statistically and practically significant difference in gross conversion, whereas the difference in net conversion are both statistically and practically insignificant. The lower bound of the 95% confidence interval is negative and beyond the negative practical significance boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation\n",
    "Gross conversion had a significant decrease both statistically and practically, which means the free trial screener has succeeded in its first aspect which was to set clearer expectations for students upfront about the course load so that they may reconsider joining the free trial, thereby improve coaches' capacity to support students who are likely to complete the course. However, looking at the confidence interval of the net conversion and the observed difference, it is very likely to cause the number of payment to decrease rather than to increase, and eventually causing a decrease in revenue. Hence, I would recommend not to launch the experiment for now. \n",
    "\n",
    "Nevertheless, having the free trial screener may benefit in a long term. It may increase the total number of student who opt for the freely available materials. Those student may want to first access the free course materials and then upgrade to paid course. This screener may still eventually increase net conversion. However, it would take a long time period in order for this to take effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='follow-up'></a>\n",
    "# Follow-Up Experiment\n",
    "\n",
    "Since the previous experiment has successfully reduced the number of enrollments without significantly reduced the number of students to continue past the free trial, next, we may run another experiment with new feature focusing on the period of free trial to test whether the retention increases. Optimizing free trial process is a long-term process, so we need to invest time and resources into different tactics to really see if they work.\n",
    "\n",
    "**Possible features:** \n",
    "- Not asking for a credit card at signup\n",
    "- Personalized e-mails\n",
    "- Experiment with different trial periods\n",
    "- Provide an early discount if they purchase the course before the free trial expiration\n",
    "\n",
    "**Null Hypothesis:** The feature does not reduce the number of frustrated students who cancel early in the course\n",
    "\n",
    "**Alternative Hypothesis:** The feature does reduce the number of frustrated students who cancel early in the course\n",
    "\n",
    "**Unit of conversion:** User-id  \n",
    "\n",
    "**Invariant metric:** Number of user-ids  \n",
    "Number of users who enroll in the free trial. This is a population sizing metric. Since the unit of diversion is user-id, we should definitely have roughly the same number of user-ids in each group.\n",
    "\n",
    "**Evalution metric:** Retention   \n",
    "The number of user-ids to remain enrolled past the 14-day boundary (and thus make at least one payment) divided by number of user-ids to complete checkout. We would expect this metric to increase if the feature successfully decreases the number of frustrated students leaving the course early.\n",
    "\n",
    "**Final thought:**  \n",
    "We could also try to increase the practical significance boundary, significance level $\\alpha$ or $\\beta$ to reduce the size as well as the duration of the experiment. The practical significance boundary of retention should be higher than the prior experiment because the follow-up experiment will be more expensive and risky. If the change is risky enough, we don’t want to expose such a large percentage of our traffic to it, we should run the experiment for a longer period with less traffic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
